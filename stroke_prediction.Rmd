---
title: "Stroke Predcition"
author: "Lin Yang, Weiheng Zhang, Fei Sun"
date: "2022/5/12"
output: pdf_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(glmnet)
library(mlbench)
library(pROC)
library(pdp)
library(vip)
library(visdat)
library(AppliedPredictiveModeling)
library(mice)
library(ROSE)
library(gridExtra)
library(mvtnorm)
library(corrplot)
library(rpart.plot)
library(e1071)
library(kernlab)
library(patchwork)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Data Cleaning

```{r  warning=FALSE, message=FALSE}
set.seed(500)
stroke <- read_csv("healthcare-dataset-stroke-data.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::select(-1) %>% 
  filter(gender == "Male" | gender == "Female" ) %>% 
  mutate(bmi = as.numeric(bmi),
         gender = as.numeric(factor(gender)) - 1,
         ever_married = as.numeric(factor(ever_married)) - 1,
         work_type = as.numeric(factor(work_type)) - 1,
         residence_type = as.numeric(factor(residence_type)) - 1,
         smoking_status = as.numeric(factor(smoking_status)) - 1,
         stroke = factor(stroke,
                         levels = c("0", "1"),
                         labels = c("neg", "pos"))) %>% 
  as.data.frame()

```

## EDA
```{r}
set.seed(500)
#prevalence of stroke
stroke %>%
  group_by(stroke) %>% 
  summarize(count = n()) %>% 
  mutate(proportion = round(count / sum(count), 3)) %>% 
  knitr::kable()

#boxplots of continuous variables
age <- ggplot(data = stroke, aes(x = stroke, y = age), group = stroke) +
  geom_boxplot() +
  labs(x = " ",
       y = "Age")

glucose <- ggplot(data = stroke, aes(x = stroke, y = avg_glucose_level), group = stroke) +
  geom_boxplot() +
  labs(x = "Stroke Status",
       y = "Average Glucose Level")

bmi <- ggplot(data = stroke, aes(x = stroke, y = bmi), group = stroke) +
  geom_boxplot() +
  labs(x = " ",
       y = "BMI")

age + glucose + bmi

#density plots of stroke vs continuous variables
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

featurePlot(x = stroke %>% dplyr::select(age, avg_glucose_level, bmi), 
            y = stroke$stroke,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

#correlation plot of predictors
corrplot::corrplot(cor(stroke[1:10]), 
         method = "circle", 
         type = "full",
         tl.cex = 0.5)
```

   
## Models

```{r  fig.height=4}
set.seed(500)
#check missing values
vis_miss(stroke)

#KNN Imputation to fill missing bmi values
knnImp <- preProcess(stroke, method = "knnImpute", k = 3)
stroke_dat <- predict(knnImp, stroke)
sapply(stroke_dat, function(x) sum(is.na(x)))
```

```{r  warning=FALSE}
set.seed(500)
stroke_dat <- ovun.sample(stroke~., data = stroke_dat, method = "under", seed = 1)$data
stroke_dat <- stroke_dat[sample(nrow(stroke_dat)),]

trainRows <- createDataPartition(y = stroke_dat$stroke, p = 0.8, list = F)

#downsample the training data
stroke_train <- stroke_dat[trainRows,]
train_x <- stroke_train[, -11]
train_y <- stroke_train$stroke

#downsample the test data
#stroke_test = ovun.sample(stroke ~ ., data = stroke_dat[-trainRows, ], method = "under", #seed = 1)$data
stroke_test <- stroke_dat[-trainRows,]
test_x = stroke_test[, -11]
test_y = stroke_test$stroke

ctrl = trainControl(method = "cv", 
                    number = 10,
                    summaryFunction = twoClassSummary, 
                    classProbs = TRUE)
```

### Penalized Logistic Regression
```{r}
set.seed(500)

glmngrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-7, -2, length = 30)))

model.glmn <- train(x = train_x,
                    y = train_y,
                    method = "glmnet",
                    tuneGrid = glmngrid,
                    metric = "ROC",
                    trControl = ctrl)

model.glmn$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))
```

### LDA
```{r}
set.seed(500)
model.lda <- train(x = train_x,
                   y = train_y,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
```

### GAM
```{r}
set.seed(500)
model.gam <- train(x = train_x,
                   y = train_y,
                   method = "gam",
                   metric = "ROC",
                   trControl = ctrl)

model.gam$finalModel
plot(model.gam$finalModel, select = 3)
```

### MARS
```{r}
set.seed(500)
model.mars = train(x = train_x,
                   y = train_y,
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:3,
                                          nprune = 2:18),
                   metric = "ROC",
                   trControl = ctrl)
plot(model.mars)
model.mars$bestTune
```


### Random Forest
```{r, warning=FALSE, dpi=300}
set.seed(500)

rf.grid <- expand.grid(mtry = 1:10,
                       splitrule = "gini",
                       min.node.size = 2:10)

rf.fit <- train(stroke ~ . ,
                stroke_train, 
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

rf.fit$bestTune

ggplot(rf.fit, highlight = TRUE)
rf.pred <- predict(rf.fit, newdata = stroke_test, type = "prob")[,2]
pred <- ifelse(rf.pred > 0.5, "pos", "neg")
confusionMatrix(data = as.factor(pred),
                reference = stroke_test$stroke,
                positive = "pos")
```

### SVM
```{r, warning=FALSE, dpi=300}
#linear kernel
set.seed(500)
svml.fit <- train(stroke ~ . , 
                  data = stroke_train, 
                  method = "svmLinear",
                  tuneGrid = data.frame(C = exp(seq(-6, -2, len = 50))),
                  trControl = ctrl)

plot(svml.fit, highlight = TRUE, xTrans = log)
svml.fit$bestTune

#radial kernel
svmr.grid <- expand.grid(C = exp(seq(-5, 2 , len = 50)),
                         sigma = exp(seq(-8, -2, len = 50)))

```

## Model Comparison
```{r}
res <- resamples(list(glmn = model.glmn, lda = model.lda, gam = model.gam, mars = model.mars, rf = rf.fit, svml = svml.fit))
roc_summary <- summary(res)$statistics[1]
roc_summary %>% knitr::kable()

bwplot(res, metric = "ROC")


rf.pred.prob <- predict(rf.fit, newdata = stroke_test, type = "prob")[, 2]
rf.pred <- ifelse(rf.pred.prob > 0.5, "pos", "neg")
confusionMatrix(data = as.factor(rf.pred),
                reference = stroke_test$stroke,
                positive = "pos")

svml.pred.prob <- predict(svml.fit, newdata = stroke_test, type = "prob")[, 2]
svml.pred <- ifelse(svml.pred.prob > 0.4, "pos", "neg")
confusionMatrix(data = as.factor(svml.pred),
                reference = stroke_test$stroke,
                positive = "pos")


roc.rf <- roc(stroke_test$stroke, rf.pred)

auc1 <- roc.rf$auc[1]
auc1
plot(roc.rf, legacy.axes = TRUE)
plot(smooth(roc.rf), col = 4, add = TRUE)
legend("bottomright", legend = paste0("rf AUC", ": ", round(auc1, 3)), cex = 1)

```


### Variable Importance
```{r}
set.seed(8106)
rf.final.per <- ranger(stroke ~ . , 
                       stroke_train, 
                       mtry = rf.fit$bestTune[[1]], 
                       min.node.size = rf.fit$bestTune[[3]],
                       splitrule = "gini",
                       importance = "permutation",
                       scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))

set.seed(8106)
rf.final.imp <- ranger(stroke ~ . , 
                       stroke_train, 
                       mtry = rf.fit$bestTune[[1]], 
                       splitrule = "gini",
                       min.node.size = rf.fit$bestTune[[3]],
                       importance = "impurity") 

barplot(sort(ranger::importance(rf.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))
```


### PDP
```{r}
pdp.rf.age <- rf.fit %>% 
  partial(pred.var = "age", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = stroke_train) +
  ggtitle("Random forest") 

pdp.rf.bmi <- rf.fit %>% 
  partial(pred.var = "bmi", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = stroke_train) +
  ggtitle("Random forest") 

pdp.rf.glucose <- rf.fit %>% 
  partial(pred.var = "healthcare-dataset-stroke-data.csvQ", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = stroke_train) +
  ggtitle("Random forest") 
```

